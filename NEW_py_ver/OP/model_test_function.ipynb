{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from POMO import OPEnv\n",
    "from POMO import OPModel\n",
    "import OPProblemDef as OP \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "depot_xy , node_xy , prize = OP.get_random_problems(2 , 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 3, 2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_params = {\n",
    "    'problem_size': 3,\n",
    "    'pomo_size': 3, \n",
    "    'embedding_dim' : 6, \n",
    "    'encoder_layer_num' : 1,\n",
    "    'head_num' : 2,\n",
    "    'qkv_dim' : 3,\n",
    "    'ff_hidden_dim' : 512}\n",
    "\n",
    "head_num = 2\n",
    "embedding_dim = 6\n",
    "batch_size = 2\n",
    "pomo_size = 3  \n",
    "\n",
    "node_index_to_pick = torch.randint(1, pomo_size + 1, (batch_size, pomo_size))\n",
    "node_index_to_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3388, 0.1809, 6.0000],\n",
       "         [0.9768, 0.9139, 5.0000],\n",
       "         [0.6260, 0.7024, 1.0000]],\n",
       "\n",
       "        [[0.0746, 0.0228, 3.0000],\n",
       "         [0.6301, 0.3931, 8.0000],\n",
       "         [0.6072, 0.4737, 7.0000]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OPModel.OPModel(**env_params)\n",
    "encoder = OPModel.OP_Encoder(**env_params)\n",
    "decoder = OPModel.OP_Decoder(**env_params)\n",
    "layer = OPModel.EncoderLayer(**env_params)\n",
    "node_xy_prize = torch.cat((node_xy, prize[:, :, None]), dim=2)\n",
    "node_xy_prize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder & encoder layer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7555, -0.0470,  0.4505, -0.4278, -0.2636, -0.2305],\n",
       "         [-2.3164, -2.1174, -2.7709, -2.2419, -0.2017,  2.3189],\n",
       "         [-2.2049, -1.4295, -2.2368, -1.5166, -0.4840,  1.2320],\n",
       "         [-0.5517, -0.1825, -0.0737,  0.3024, -0.6495, -0.5718]],\n",
       "\n",
       "        [[ 1.0892, -0.4807,  0.3135, -0.4625, -0.4893,  0.0723],\n",
       "         [-1.0763, -1.1824, -1.1495, -0.8783, -0.3263,  0.9665],\n",
       "         [-3.1878, -2.6839, -3.8434, -3.1061, -0.1486,  3.1247],\n",
       "         [-2.8128, -2.3284, -3.3367, -2.6341, -0.2409,  2.5987]]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_depot = encoder.embedding_depot(depot_xy)\n",
    "embedded_node = encoder.embedding_node(node_xy_prize)\n",
    "out_first = torch.cat((embedded_depot, embedded_node), dim=1)\n",
    "out_first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7555, -0.0470,  0.4505, -0.4278, -0.2636, -0.2305],\n",
       "         [-2.3164, -2.1174, -2.7709, -2.2419, -0.2017,  2.3189],\n",
       "         [-2.2049, -1.4295, -2.2368, -1.5166, -0.4840,  1.2320],\n",
       "         [-0.5517, -0.1825, -0.0737,  0.3024, -0.6495, -0.5718]],\n",
       "\n",
       "        [[ 1.0892, -0.4807,  0.3135, -0.4625, -0.4893,  0.0723],\n",
       "         [-1.0763, -1.1824, -1.1495, -0.8783, -0.3263,  0.9665],\n",
       "         [-3.1878, -2.6839, -3.8434, -3.1061, -0.1486,  3.1247],\n",
       "         [-2.8128, -2.3284, -3.3367, -2.6341, -0.2409,  2.5987]]],\n",
       "       grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.forward(depot_xy,node_xy_prize)\n",
    "encoder.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1899, -0.1847, -0.3827],\n",
       "          [ 1.3745,  0.3794,  1.0963],\n",
       "          [ 1.1744,  0.5815,  1.0508],\n",
       "          [ 0.1374,  0.4218,  0.2831]],\n",
       "\n",
       "         [[ 0.1644, -0.1034,  0.0605],\n",
       "          [-1.7196,  1.7423, -0.2025],\n",
       "          [-1.2781,  1.0751, -0.4079],\n",
       "          [ 0.1488, -0.2609, -0.2219]]],\n",
       "\n",
       "\n",
       "        [[[-0.2222, -0.2559, -0.4364],\n",
       "          [ 0.5971,  0.2595,  0.5206],\n",
       "          [ 1.8984,  0.4943,  1.5002],\n",
       "          [ 1.6581,  0.4879,  1.3324]],\n",
       "\n",
       "         [[ 0.2075,  0.1699,  0.1980],\n",
       "          [-0.6499,  0.7409, -0.0632],\n",
       "          [-2.4161,  2.3564, -0.3299],\n",
       "          [-2.0630,  1.9948, -0.3254]]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = OPModel.reshape_by_heads(layer.Wq(out_first), head_num=head_num)\n",
    "k = OPModel.reshape_by_heads(layer.Wk(out_first), head_num=head_num)\n",
    "v = OPModel.reshape_by_heads(layer.Wv(out_first), head_num=head_num)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1899, -0.1847, -0.3827],\n",
       "          [ 1.3745,  0.3794,  1.0963],\n",
       "          [ 1.1744,  0.5815,  1.0508],\n",
       "          [ 0.1374,  0.4218,  0.2831]],\n",
       "\n",
       "         [[ 0.1644, -0.1034,  0.0605],\n",
       "          [-1.7196,  1.7423, -0.2025],\n",
       "          [-1.2781,  1.0751, -0.4079],\n",
       "          [ 0.1488, -0.2609, -0.2219]]],\n",
       "\n",
       "\n",
       "        [[[-0.2222, -0.2559, -0.4364],\n",
       "          [ 0.5971,  0.2595,  0.5206],\n",
       "          [ 1.8984,  0.4943,  1.5002],\n",
       "          [ 1.6581,  0.4879,  1.3324]],\n",
       "\n",
       "         [[ 0.2075,  0.1699,  0.1980],\n",
       "          [-0.6499,  0.7409, -0.0632],\n",
       "          [-2.4161,  2.3564, -0.3299],\n",
       "          [-2.0630,  1.9948, -0.3254]]]], grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.forward(encoder.output)\n",
    "layer.qclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1275, -0.7520, -0.2116,  0.8838,  0.5310, -1.1457],\n",
       "         [-0.1127, -0.9562, -0.2574,  0.7736,  0.4787, -1.0108],\n",
       "         [-0.1295, -0.7263, -0.2051,  0.8262,  0.5044, -1.0765],\n",
       "         [-0.1501, -0.4398, -0.1419,  0.8765,  0.5182, -1.1334]],\n",
       "\n",
       "        [[-0.1568, -1.4425, -0.5099,  1.2337,  0.9711, -1.7452],\n",
       "         [-0.1653, -1.3810, -0.5040,  1.2100,  0.9536, -1.7145],\n",
       "         [-0.1041, -1.8610, -0.5553,  0.9655,  0.7698, -1.3992],\n",
       "         [-0.1176, -1.7500, -0.5428,  1.0209,  0.8112, -1.4707]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_out_concat = OPModel.multi_head_attention(q,k,v)\n",
    "multi_out_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2006,  0.2907, -0.6694,  0.4338,  0.1894, -0.3933],\n",
       "         [ 0.0908,  0.3160, -0.5903,  0.5149,  0.2114, -0.4005],\n",
       "         [ 0.1987,  0.2750, -0.6486,  0.4231,  0.1816, -0.3739],\n",
       "         [ 0.3267,  0.2220, -0.7102,  0.3086,  0.1430, -0.3355]],\n",
       "\n",
       "        [[ 0.0784,  0.5062, -0.8357,  0.7446,  0.3003, -0.5935],\n",
       "         [ 0.0979,  0.4897, -0.8329,  0.7208,  0.2890, -0.5741],\n",
       "         [-0.1796,  0.5604, -0.6282,  0.9036,  0.3583, -0.6110],\n",
       "         [-0.1159,  0.5440, -0.6748,  0.8613,  0.3423, -0.6024]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_out = layer.multi_head_combine(multi_out_concat)\n",
    "multi_head_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3964,  1.0797,  1.1920,  0.6096,  0.7141, -0.8170],\n",
       "         [-1.0297, -1.3418, -1.1587, -1.2875,  1.1302,  1.4039],\n",
       "         [-0.8625, -0.5758, -0.8028, -0.5940, -0.4166,  0.4776],\n",
       "         [ 0.4958,  0.8379,  0.7694,  1.2719, -1.4277, -1.0646]],\n",
       "\n",
       "        [[ 1.4995,  1.3637,  1.3995,  1.1881, -1.4044, -1.3276],\n",
       "         [ 0.3051,  0.5260,  0.4809,  0.7684, -0.3909, -0.5772],\n",
       "         [-1.0244, -1.1426, -1.0850, -1.1833,  1.2593,  1.1649],\n",
       "         [-0.7803, -0.7471, -0.7955, -0.7732,  0.5360,  0.7399]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = layer.add_n_normalization_1(out_first, multi_head_out)\n",
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3064, -0.0495, -0.3178, -0.5758, -0.2116, -0.1240],\n",
       "         [-0.0327,  0.4479, -0.0673,  0.3292,  0.0288, -0.1712],\n",
       "         [-0.1008,  0.1719, -0.1633,  0.2304, -0.0434, -0.2140],\n",
       "         [-0.3560,  0.0581, -0.3685, -0.4500, -0.3415, -0.2802]],\n",
       "\n",
       "        [[-0.4701, -0.0512, -0.5074, -0.8080, -0.4498, -0.2647],\n",
       "         [-0.2437,  0.0329, -0.2432, -0.2951, -0.2312, -0.1803],\n",
       "         [-0.0258,  0.4075, -0.0621,  0.2584,  0.0222, -0.2232],\n",
       "         [-0.0671,  0.2645, -0.0881,  0.1935, -0.0229, -0.2170]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = layer.feed_forward(out1)\n",
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4628,  1.0573,  1.2432,  0.2325,  0.5848, -0.7320],\n",
       "         [-0.9798, -1.2727, -1.1230, -1.3016,  1.1806,  1.4078],\n",
       "         [-0.8674, -0.6794, -0.8302, -0.3820, -0.2886,  0.4538],\n",
       "         [ 0.3844,  0.8948,  0.7100,  1.4511, -1.4768, -1.1296]],\n",
       "\n",
       "        [[ 1.4924,  1.3982,  1.3485,  0.9003, -1.4212, -1.3586],\n",
       "         [ 0.3190,  0.4813,  0.5588,  1.0552, -0.3812, -0.5313],\n",
       "         [-1.0286, -1.0933, -1.1126, -1.2640,  1.2255,  1.1524],\n",
       "         [-0.7828, -0.7862, -0.7946, -0.6915,  0.5769,  0.7375]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3 = layer.add_n_normalization_2(out1, out2)\n",
    "out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4628,  1.0573,  1.2432,  0.2325,  0.5848, -0.7320],\n",
       "         [-0.9798, -1.2727, -1.1230, -1.3016,  1.1806,  1.4078],\n",
       "         [-0.8674, -0.6794, -0.8302, -0.3820, -0.2886,  0.4538],\n",
       "         [ 0.3844,  0.8948,  0.7100,  1.4511, -1.4768, -1.1296]],\n",
       "\n",
       "        [[ 1.4924,  1.3982,  1.3485,  0.9003, -1.4212, -1.3586],\n",
       "         [ 0.3190,  0.4813,  0.5588,  1.0552, -0.3812, -0.5313],\n",
       "         [-1.0286, -1.0933, -1.1126, -1.2640,  1.2255,  1.1524],\n",
       "         [-0.7828, -0.7862, -0.7946, -0.6915,  0.5769,  0.7375]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.forward(encoder.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   why this cell is not equal to next two cells????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3521,  1.0348,  1.1514,  0.4580, -0.2703, -0.9326],\n",
       "         [-1.0772, -1.3681, -1.1673, -1.3748,  1.4683,  1.3704],\n",
       "         [-0.8352, -0.5412, -0.8019, -0.4017,  0.1268,  0.5414],\n",
       "         [ 0.5602,  0.8745,  0.8178,  1.3186, -1.3248, -0.9793]],\n",
       "\n",
       "        [[ 1.5184,  1.4240,  1.4832,  0.8800, -1.5461, -1.3420],\n",
       "         [ 0.2641,  0.4301,  0.3432,  1.0632, -0.1901, -0.5619],\n",
       "         [-1.0258, -1.1117, -0.9988, -1.2936,  1.0498,  1.1403],\n",
       "         [-0.7567, -0.7424, -0.8276, -0.6496,  0.6864,  0.7636]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in encoder.layers:\n",
    "    output = layer(encoder.output)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3521,  1.0348,  1.1514,  0.4580, -0.2703, -0.9326],\n",
       "         [-1.0772, -1.3681, -1.1673, -1.3748,  1.4683,  1.3704],\n",
       "         [-0.8352, -0.5412, -0.8019, -0.4017,  0.1268,  0.5414],\n",
       "         [ 0.5602,  0.8745,  0.8178,  1.3186, -1.3248, -0.9793]],\n",
       "\n",
       "        [[ 1.5184,  1.4240,  1.4832,  0.8800, -1.5461, -1.3420],\n",
       "         [ 0.2641,  0.4301,  0.3432,  1.0632, -0.1901, -0.5619],\n",
       "         [-1.0258, -1.1117, -0.9988, -1.2936,  1.0498,  1.1403],\n",
       "         [-0.7567, -0.7424, -0.8276, -0.6496,  0.6864,  0.7636]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.forward(depot_xy,node_xy_prize) #out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder set kv test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.__init__(**model_params)\n",
    "# model.decoder.set_kv(model.encoded_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0744, -0.6503,  0.0306, -0.1368,  0.2379,  0.9080]],\n",
       "\n",
       "        [[ 0.1665, -0.9202,  0.0944, -0.2593,  0.4848,  1.2750]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_depot = nn.Linear(2, embedding_dim)\n",
    "embedded_depott = embedding_depot(depot_xy)\n",
    "embedded_depott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=6, bias=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_depot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9004, -0.8712,  2.2221,  2.8133, -1.0249,  1.8653],\n",
       "         [ 1.1746, -0.8182,  2.3426,  2.2107, -0.7013,  1.6274],\n",
       "         [ 0.2805, -0.3303,  0.8675,  0.6956, -0.4095,  0.4266]],\n",
       "\n",
       "        [[ 1.2299, -0.5047,  1.1158,  1.6769, -0.8067,  0.9642],\n",
       "         [ 2.2761, -1.1309,  3.0290,  3.5392, -1.1286,  2.4807],\n",
       "         [ 1.9782, -0.9996,  2.7332,  3.1270, -1.0403,  2.1692]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_node = nn.Linear(3, embedding_dim)\n",
    "eembedded_node = embedding_node(node_xy_prize)\n",
    "eembedded_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0744, -0.6503,  0.0306, -0.1368,  0.2379,  0.9080],\n",
       "         [ 1.9004, -0.8712,  2.2221,  2.8133, -1.0249,  1.8653],\n",
       "         [ 1.1746, -0.8182,  2.3426,  2.2107, -0.7013,  1.6274],\n",
       "         [ 0.2805, -0.3303,  0.8675,  0.6956, -0.4095,  0.4266]],\n",
       "\n",
       "        [[ 0.1665, -0.9202,  0.0944, -0.2593,  0.4848,  1.2750],\n",
       "         [ 1.2299, -0.5047,  1.1158,  1.6769, -0.8067,  0.9642],\n",
       "         [ 2.2761, -1.1309,  3.0290,  3.5392, -1.1286,  2.4807],\n",
       "         [ 1.9782, -0.9996,  2.7332,  3.1270, -1.0403,  2.1692]]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_nodes = torch.cat((embedded_depott, eembedded_node), dim=1)\n",
    "encoded_nodes\n",
    "#shape: (batch, problem+1, embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 3, 2]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = node_index_to_pick.size(0)\n",
    "pomo_size = node_index_to_pick.size(1)\n",
    "embedding_dim = encoded_nodes.size(2)\n",
    "node_index_to_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1],\n",
       "         [2, 2, 2, 2, 2, 2],\n",
       "         [3, 3, 3, 3, 3, 3]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1],\n",
       "         [3, 3, 3, 3, 3, 3],\n",
       "         [2, 2, 2, 2, 2, 2]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gathering_index = node_index_to_pick[:, :, None].expand(batch_size, pomo_size, embedding_dim)\n",
    "node_index_to_pick[:, :, None].expand(batch_size, pomo_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9004, -0.8712,  2.2221,  2.8133, -1.0249,  1.8653],\n",
       "         [ 1.1746, -0.8182,  2.3426,  2.2107, -0.7013,  1.6274],\n",
       "         [ 0.2805, -0.3303,  0.8675,  0.6956, -0.4095,  0.4266]],\n",
       "\n",
       "        [[ 1.2299, -0.5047,  1.1158,  1.6769, -0.8067,  0.9642],\n",
       "         [ 1.9782, -0.9996,  2.7332,  3.1270, -1.0403,  2.1692],\n",
       "         [ 2.2761, -1.1309,  3.0290,  3.5392, -1.1286,  2.4807]]],\n",
       "       grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n",
    "picked_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = torch.arange(start=1, end=pomo_size+1)[None, :].expand(batch_size, pomo_size)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor shape: torch.Size([2, 3, 8])\n",
      "Reshaped tensor shape: torch.Size([2, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with shape (2, 3, 8)\n",
    "qkv = torch.randn(2, 3, 8)\n",
    "batch_s = qkv.size(0)\n",
    "n = qkv.size(1)\n",
    "head_num = 2\n",
    "\n",
    "# Reshape the tensor with -1 in the last dimension\n",
    "q_reshaped = qkv.reshape(batch_s, n, head_num, -1)\n",
    "\n",
    "print(\"Original tensor shape:\", qkv.shape)\n",
    "print(\"Reshaped tensor shape:\", q_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6614,  0.1773, -2.3643,  0.3933, -0.9040, -1.9822,  0.8090,\n",
       "          -0.8590],\n",
       "         [ 0.0983,  1.6852,  1.2493, -0.8066,  0.0875, -0.9733,  0.7543,\n",
       "          -0.0772],\n",
       "         [-0.9903, -0.0448,  0.7174, -0.8195, -0.1929, -0.9320, -2.2059,\n",
       "           0.5364]],\n",
       "\n",
       "        [[-0.2947,  0.0473,  0.4203,  1.0884, -0.8996, -1.3518, -0.0557,\n",
       "          -0.3800],\n",
       "         [ 0.7307, -2.4862, -0.2487,  0.3316,  1.2053, -0.3663,  1.7566,\n",
       "           0.6264],\n",
       "         [-2.3257,  0.3481,  0.6272, -0.8287,  1.7677,  0.2287,  0.1635,\n",
       "          -0.4741]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.6614,  0.1773, -2.3643,  0.3933],\n",
       "          [-0.9040, -1.9822,  0.8090, -0.8590]],\n",
       "\n",
       "         [[ 0.0983,  1.6852,  1.2493, -0.8066],\n",
       "          [ 0.0875, -0.9733,  0.7543, -0.0772]],\n",
       "\n",
       "         [[-0.9903, -0.0448,  0.7174, -0.8195],\n",
       "          [-0.1929, -0.9320, -2.2059,  0.5364]]],\n",
       "\n",
       "\n",
       "        [[[-0.2947,  0.0473,  0.4203,  1.0884],\n",
       "          [-0.8996, -1.3518, -0.0557, -0.3800]],\n",
       "\n",
       "         [[ 0.7307, -2.4862, -0.2487,  0.3316],\n",
       "          [ 1.2053, -0.3663,  1.7566,  0.6264]],\n",
       "\n",
       "         [[-2.3257,  0.3481,  0.6272, -0.8287],\n",
       "          [ 1.7677,  0.2287,  0.1635, -0.4741]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a sample tensor\n",
    "score_scaled = torch.randn(2, 3, 2, 5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4972, 0.3680, 0.7126, 0.8183, 0.1567],\n",
       "          [0.5028, 0.6320, 0.2874, 0.1817, 0.8433]],\n",
       "\n",
       "         [[0.3735, 0.1331, 0.8548, 0.4177, 0.6178],\n",
       "          [0.6265, 0.8669, 0.1452, 0.5823, 0.3822]],\n",
       "\n",
       "         [[0.4067, 0.6479, 0.7120, 0.8668, 0.3618],\n",
       "          [0.5933, 0.3521, 0.2880, 0.1332, 0.6382]]],\n",
       "\n",
       "\n",
       "        [[[0.0961, 0.7148, 0.1431, 0.6722, 0.1692],\n",
       "          [0.9039, 0.2852, 0.8569, 0.3278, 0.8308]],\n",
       "\n",
       "         [[0.3202, 0.2017, 0.5763, 0.8304, 0.6945],\n",
       "          [0.6798, 0.7983, 0.4237, 0.1696, 0.3055]],\n",
       "\n",
       "         [[0.8382, 0.8874, 0.0959, 0.3761, 0.4487],\n",
       "          [0.1618, 0.1126, 0.9041, 0.6239, 0.5513]]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "softmax = nn.Softmax(dim=2)\n",
    "softmax3 = nn.Softmax(dim=3)\n",
    "weights = softmax(score_scaled)\n",
    "weights3 = softmax3(score_scaled)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1562, 0.1346, 0.4731, 0.2068, 0.0293],\n",
       "          [0.2015, 0.2951, 0.2434, 0.0586, 0.2015]],\n",
       "\n",
       "         [[0.2863, 0.0715, 0.3823, 0.1425, 0.1173],\n",
       "          [0.3745, 0.3634, 0.0506, 0.1549, 0.0566]],\n",
       "\n",
       "         [[0.2430, 0.0501, 0.4155, 0.2405, 0.0509],\n",
       "          [0.5239, 0.0402, 0.2484, 0.0546, 0.1328]]],\n",
       "\n",
       "\n",
       "        [[[0.0945, 0.3506, 0.0707, 0.4263, 0.0579],\n",
       "          [0.4573, 0.0719, 0.2177, 0.1069, 0.1461]],\n",
       "\n",
       "         [[0.1185, 0.0328, 0.2193, 0.4847, 0.1447],\n",
       "          [0.3567, 0.1842, 0.2285, 0.1403, 0.0903]],\n",
       "\n",
       "         [[0.3200, 0.4306, 0.0251, 0.0988, 0.1255],\n",
       "          [0.0920, 0.0814, 0.3526, 0.2443, 0.2298]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1956, -0.3440,  0.9126,  0.0849, -1.8680],\n",
       "          [-0.1846,  0.1969,  0.0045, -1.4200, -0.1847]],\n",
       "\n",
       "         [[ 0.2441, -1.1426,  0.5334, -0.4535, -0.6479],\n",
       "          [ 0.7615,  0.7314, -1.2391, -0.1212, -1.1281]],\n",
       "\n",
       "         [[ 0.8197, -0.7600,  1.3562,  0.8093, -0.7425],\n",
       "          [ 1.1974, -1.3699,  0.4510, -1.0633, -0.1747]]],\n",
       "\n",
       "\n",
       "        [[[-0.7472,  0.5636, -1.0379,  0.7592, -1.2381],\n",
       "          [ 1.4944, -0.3554,  0.7523,  0.0410,  0.3534]],\n",
       "\n",
       "         [[-0.1279, -1.4122,  0.4874,  1.2804,  0.0718],\n",
       "          [ 0.6249, -0.0363,  0.1797, -0.3083, -0.7493]],\n",
       "\n",
       "         [[ 1.1031,  1.4000, -1.4427, -0.0717,  0.1672],\n",
       "          [-0.5417, -0.6647,  0.8013,  0.4343,  0.3732]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1524, -1.7431, -1.6052, -0.8216],\n",
      "        [ 0.3264, -3.0486, -0.3137, -2.6651],\n",
      "        [ 4.3786,  0.7942,  0.7758, -1.8524]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a sample tensor\n",
    "tensor = torch.randn(2, 3, 4)  # Tensor of shape (2, 3, 4)\n",
    "\n",
    "# Apply a sum operation along dimension 1\n",
    "sum_result = torch.sum(tensor, dim=0)\n",
    "\n",
    "print(sum_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0783, -1.0028,  0.8775,  1.3500],\n",
       "         [-0.6401, -0.2642,  2.3366,  1.2663],\n",
       "         [ 1.5993, -0.4154,  1.3792,  0.7551],\n",
       "         [ 2.4278, -1.0157, -1.4257,  0.4554],\n",
       "         [-3.1709, -0.4085, -0.8739,  0.2029]],\n",
       "\n",
       "        [[ 1.4407, -0.3782,  0.9619, -1.2570],\n",
       "         [ 0.6438,  0.7857, -0.0149, -2.8753],\n",
       "         [-0.7831,  0.5508,  0.1061,  0.9683],\n",
       "         [ 0.7241,  0.2215,  1.4741,  0.8375],\n",
       "         [ 1.6220, -0.3577,  0.2856, -0.8859]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "m = nn.InstanceNorm1d(5, affine=True)\n",
    "input = torch.randn(2, 5, 4)\n",
    "output = m(input)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2775, -1.4901,  0.6188,  1.1488],\n",
       "         [-1.0992, -0.7850,  1.3895,  0.4947],\n",
       "         [ 0.9835, -1.5907,  0.7023, -0.0951],\n",
       "         [ 1.5349, -0.7459, -1.0175,  0.2285],\n",
       "         [-1.6526,  0.5127,  0.1479,  0.9920]],\n",
       "\n",
       "        [[ 1.1675, -0.5330,  0.7199, -1.3545],\n",
       "         [ 0.6816,  0.7774,  0.2366, -1.6956],\n",
       "         [-1.5294,  0.5238, -0.1608,  1.1664],\n",
       "         [-0.2022, -1.3293,  1.4796,  0.0519],\n",
       "         [ 1.5532, -0.5586,  0.1276, -1.1222]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
